# -*- coding: utf-8 -*-
"""Extension2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGAAcJwlIAykUABRCUFpQnTrQuXUuWV0
"""

!pip install -U "transformers>=4.39.0"
!pip install peft bitsandbytes
!pip install -U "trl==0.8.3"

import torch
from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig
from trl import SFTTrainer
from peft import LoraConfig

model_id = "llava-hf/llava-1.5-7b-hf"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
)

model = LlavaForConditionalGeneration.from_pretrained(model_id,
                                                      quantization_config=quantization_config,
                                                      torch_dtype=torch.float16)

LLAVA_CHAT_TEMPLATE = """A chat between a curious user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user's questions.
{% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}
ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}
<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}"""

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.chat_template = LLAVA_CHAT_TEMPLATE
processor = AutoProcessor.from_pretrained(model_id)
processor.tokenizer = tokenizer

class LLavaDataCollator:
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, examples):
        texts = []
        images = []
        for example in examples:
            messages = example["messages"]
            text = self.processor.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            texts.append(text)
            images.append(example["images"])

        batch = self.processor(texts, images, return_tensors="pt", padding=True)

        labels = batch["input_ids"].clone()
        if self.processor.tokenizer.pad_token_id is not None:
            labels[labels == self.processor.tokenizer.pad_token_id] = -100
        batch["labels"] = labels

        return batch

data_collator = LLavaDataCollator(processor)

from datasets import Dataset, DatasetDict
import json
import os
from PIL import Image
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

image_folder = "/content/drive/MyDrive/NLP_Final_Project/Data/train_dev_images"

from datasets import load_from_disk

train_dataset = load_from_disk("/content/drive/MyDrive/NLP_Final_Project/Milestone_4/fixed_training_data_2.0")
eval_dataset = load_from_disk("/content/drive/MyDrive/NLP_Final_Project/Milestone_4/fixed_eval_data_2.0")

print(train_dataset)
print(eval_dataset)

train_dataset[0]

# training_args = TrainingArguments(
#     output_dir="llava-1.5-7b-hf-ft-mix-vsft",
#     report_to="tensorboard",
#     learning_rate=1.4e-5,
#     per_device_train_batch_size=8,
#     gradient_accumulation_steps=1,
#     logging_steps=5,
#     num_train_epochs=1,
#     evaluation_strategy="steps",      # Evaluate periodically
#     eval_steps=5,                   # Evaluate every 100 steps
#     push_to_hub=True,
#     gradient_checkpointing=True,
#     remove_unused_columns=False,
#     fp16=True,
#     bf16=False
# )

# TrainingArguments configuration
training_args = TrainingArguments(
    output_dir="llava-1.5-7b-hf-ft-mix-vsft",
    report_to="tensorboard",
    learning_rate=1.4e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    logging_steps=5,                  # Log metrics every 5 steps
    num_train_epochs=8,               # Number of epochs
    evaluation_strategy="epoch",      # Evaluate periodically
    eval_steps=500,                   # Evaluate every 100 steps
    save_steps=500,                   # Save checkpoint every 500 steps
    save_total_limit=2,               # Keep only the last 2 checkpoints
    gradient_checkpointing=True,
    remove_unused_columns=False,
    fp16=True,
    push_to_hub=True
)

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules="all-linear"
)

from huggingface_hub import login

from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    predictions, references = eval_pred

    # Ensure predictions are strings (decode if logits are provided)
    if isinstance(predictions, tuple):  # If logits are provided
        predictions = predictions[0]  # Extract logits
    if hasattr(predictions, "argmax"):  # If logits exist, get class indices
        predictions = predictions.argmax(axis=-1)

    if isinstance(predictions[0], list):
        predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]

    # Compare predictions to ground truth labels
    accuracy = accuracy_score(references, predictions)

    # Optional: Print some examples for manual verification
    for i in range(5):  # Show the first 5 predictions and references
        print(f"Prediction: {predictions[i]}")
        print(f"Reference: {references[i]}\n")

    return {"accuracy": accuracy}

# from trl import SFTTrainer, SFTConfig

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    dataset_text_field="text",  # need a dummy field
    tokenizer=tokenizer,
    data_collator=data_collator,
    dataset_kwargs={"skip_prepare_dataset": True},
)

trainer.train()

# # Save the fine-tuned model
# output_dir = "/content/drive/MyDrive/5000_Trained_Model"  # Define a directory to save your model
# trainer.save_model(output_dir)

from transformers import AutoProcessor

# Define the output directory
output_dir = "/content/drive/MyDrive/5000_Trained_Model"

# Save the model
trainer.save_model(output_dir)

# Save the processor (if it's a multimodal model)
processor.save_pretrained(output_dir)

# Save the tokenizer (associated with the processor)
processor.tokenizer.save_pretrained(output_dir)

print(f"Model, processor, and tokenizer saved to {output_dir}.")

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# inputs = processor(images=test_image, text=prompt1, return_tensors="pt")
# inputs = {key: value.to(device) for key, value in inputs.items()}

import json
from PIL import Image

# Define paths
question_file_path = "/content/drive/My Drive/NLP_Final_Project/Data/selected_test_questions.json"
image_dir = "/content/drive/My Drive/NLP_Final_Project/Data/test_images/"
output_file_path = "/content/drive/My Drive/NLP_Final_Project/m4_8epoch_finetuned_predictions05.json"

# Load the question file
with open(question_file_path, "r") as f:
    questions = json.load(f)

# Initialize a list to store all responses
response_data = []

# Iterate through all questions
for idx, question in enumerate(questions, start=1):
    print(f"Processing question {idx} out of {len(questions)}...")

    question_id = question["imageId"]
    question_text = question["question"]

    # Load the corresponding image
    image_path = f"{image_dir}{question_id}.jpg"
    try:
        test_image = Image.open(image_path)
    except FileNotFoundError:
        print(f"Image for image ID {question_id} not found. Skipping.")
        continue

    # Prompt 1: Generate Sub-questions
    prompt1 = (
        f"USER: <image>\n"
        "Question: What piece of furniture is to the right of the pink blanket?\n"
        "Sub-questions: Where in the image are the blankets?\n"
        "Where in the image is the pink blanket?\n"
        "Where to the right of the pink blanket is the piece of furniture?\n"
        "Following the format of the above examples, generate a set of sub-questions for the question and then answer the sub-questions using the provided image.\n"
        f"Question: {question_text}\nSub-questions: ASSISTANT:" )

    # Generate response for prompt1
    inputs = processor(images=test_image, text=prompt1, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs
    )
    response_to_prompt1 = processor.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    if "ASSISTANT:" in response_to_prompt1:
        response_to_prompt1 = response_to_prompt1.split("ASSISTANT:")[-1].strip()


    # Parse sub-questions using '?' and '.'
    sub_questions = response_to_prompt1.replace("?", ".").split(".")  # Normalize punctuation and split by '.'
    sub_questions = [q.strip() for q in sub_questions if q.strip()]  # Clean and remove empty entries


    qa_context = ""

    # Iterate through each sub-question
    for sub_idx, sub_question in enumerate(sub_questions, start=1):

        # Prompt for answering a single sub-question
        prompt2 = (
            f"USER: <image>\nQuestion: {sub_question}\nASSISTANT:"
        )

        # Generate response for sub-question
        inputs = processor(images=test_image, text=prompt2, return_tensors="pt").to("cuda")
        outputs = model.generate(
            **inputs
        )
        sub_answer = processor.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        if "ASSISTANT:" in sub_answer:
            sub_answer = sub_answer.split("ASSISTANT:")[-1].strip()


        qa_context += f"Q: {sub_question}\nA: {sub_answer}\n"


    # Final Prompt: Use Q&A context to answer the main question
    prompt3 = (
        f"USER: <image>\nContext:\n{qa_context}\n"
        f"Use the image and above context to answer the following question:\n{question_text}\n"
        "Provide a short answer.\nASSISTANT:"
    )

    # Generate final answer
    inputs = processor(images=test_image, text=prompt3, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs
    )
    final_answer = processor.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    if "ASSISTANT:" in final_answer:
        final_answer = final_answer.split("ASSISTANT:")[-1].strip()

    print(f"Final Answer: {final_answer}")

    # Save response
    response_data.append({
        "questionId": question_id,
        "sub_questions": qa_context,
        "prediction": final_answer,
    })


# Save all responses to JSON
with open(output_file_path, "w") as f:
    json.dump(response_data, f, indent=4)

print(f"All responses saved to {output_file_path}")

#####







