# -*- coding: utf-8 -*-
"""final_strong_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JmbpzITsXBwlxcyuqIgnNH0988pd5C3F

Re-doing strong baseline experiments based on best performing extension 1 experiment
"""

!pip install transformers
!pip install transformers torch torchvision accelerate sentencepiece

import torch
print(torch.__version__)

import torch
print(torch.cuda.is_available())

# Ensure necessary imports
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

# Specify the model identifier
model_id = "llava-hf/llava-1.5-7b-hf"

# Load the processor and model
processor = AutoProcessor.from_pretrained(model_id)
model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

print(model)

from PIL import Image
import requests
from io import BytesIO

from google.colab import drive
drive.mount('/content/drive')

test_image1 = Image.open("/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Data/test_images/90.jpg")
prompt = "USER: <image>\nWhat do you see in this image?\nASSISTANT:"


inputs = processor(images=test_image1, text=prompt, return_tensors="pt").to("cuda")
outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
)

response = processor.decode(outputs[0], skip_special_tokens=True)
print(response)

"""# Run strong baseline"""

import json
from PIL import Image
import re
from google.colab import drive

# Set the seed for reproducibility
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Mount Google Drive
drive.mount('/content/drive')

# Define paths
question_file_path = "/content/drive/MyDrive/CIS 5300/Final Project/Data/selected_test_questions.json"
image_dir = "/content/drive/MyDrive/CIS 5300/Final Project/Data/test_images/"
output_file_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_simplebaselinedefault_predictions.json"

# Load the question file
with open(question_file_path, "r") as f:
    questions = json.load(f)

# Helper function to clean subquestions
def extract_valid_subquestions(subquestion_text):
    # Find all indices of '.' or '?' in the text
    delimiters = [m.start() for m in re.finditer(r'[.?]', subquestion_text)]
    valid_subquestions = []

    start = 0
    for end in delimiters:  # Iterate through all delimiters
        subquestion = subquestion_text[start:end + 1].strip()  # Include the delimiter
        if subquestion:  # Ensure it's not empty
            x = re.findall("^[0-9]+.$", subquestion)
            y = re.findall("^Sub-questions:[\s\n]+[0-9]+.$", subquestion)
            if not (x or y):
              valid_subquestions.append(subquestion)
        start = end + 1  # Update start for the next subquestion

    return valid_subquestions

# Initialize a list to store all responses
response_data = []

# Iterate through all questions
for idx, question in enumerate(questions, start=1):
    print(f"Processing question {idx} out of {len(questions)}...")

    question_id = question["imageId"]
    question_text = question["question"]

    # Load the corresponding image
    image_path = f"{image_dir}{question_id}.jpg"
    try:
        test_image = Image.open(image_path)
    except FileNotFoundError:
        print(f"Image for image ID {question_id} not found. Skipping.")
        continue

    # Prompt 1 - sub-question generation
    prompt1 = (
        f"USER: <image>\n"
        "Question: What piece of furniture is to the right of the pink blanket? "
        "Sub-questions: Where in the image is the pink blanket? Describe what is to the right of the pink blanket. Where to the right of the pink blanket is the piece of furniture? Describe the piece of furniture. What piece of furniture is it?\n"
        f"Following the format of the above example, break down the following question into sub-questions. Question: {question_text} ASSISTANT:"
    )

    # Generate response for prompt1
    inputs = processor(images=test_image, text=prompt1, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )
    response_to_prompt1 = processor.decode(outputs[0], skip_special_tokens=True).strip()
    if "ASSISTANT:" in response_to_prompt1:
        response_to_prompt1 = response_to_prompt1.split("ASSISTANT:")[-1].strip()

    print(f"Response to Prompt 1: {response_to_prompt1}")

    # Extract subquestions from response to prompt 1:
    valid_subquestions = extract_valid_subquestions(response_to_prompt1)
    if not valid_subquestions:
        print(f"No valid subquestions for ImageID: {question_id}")

        # Use best performing simple baseline prompt with formatting instruction if no valid subquestions
        final_prompt = f"USER: <image>\n{question_text} Provide a short answer.\nASSISTANT:"
        inputs = processor(images=test_image, text=final_prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)
        final_answer = processor.decode(outputs[0], skip_special_tokens=True).split("ASSISTANT:")[-1].strip()

        print(f"Official Question: {question_text}")
        print(f"LLaVA Prediction: {final_answer}\n")

        response_data.append({
        "questionId": question_id,
        "subquestions": response_to_prompt1,
        "error": "No valid subquestions",
        "prediction": final_answer
        })
        continue

    # Initialize context
    context = ""

    # Intermediate prompts - answer-to-subquestions generation. Process each valid subquestion
    subquestion_responses = []
    for sub_idx, subquestion in enumerate(valid_subquestions, start=1):
        prompt = f"USER: <image>\nQuestion: {subquestion}\nASSISTANT:"
        inputs = processor(images=test_image, text=prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.9)
        response = processor.decode(outputs[0], skip_special_tokens=True).split("ASSISTANT:")[-1].strip()
        subquestion_responses.append({"subquestion": subquestion, "response": response})
        print(f"Subquestion {sub_idx}: {subquestion}")
        print(f"LLaVA Response: {response}\n")

        # Update context text (but not image)
        context += f" {subquestion} {response}"

    # Final prompt: Ask the official question with context and image
    final_prompt = f"USER: <image>\nQuestion: Use the context: {context}. Answer the following question: {question_text} Provide a short answer.\nASSISTANT:"
    inputs = processor(images=test_image, text=final_prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)
    final_answer = processor.decode(outputs[0], skip_special_tokens=True).split("ASSISTANT:")[-1].strip()

    print(f"Official Question: {question_text}")
    print(f"LLaVA Prediction: {final_answer}\n")

    # Save response
    response_data.append({
        "questionId": question_id,
        "subquestions": response_to_prompt1,  # Response to Prompt 1
        "subquestionResponses": subquestion_responses, # Reponse to intermediate prompts
        "prediction": final_answer,  # Response to final prompt
    })

    print("-" * 50)

# Save all responses to JSON
with open(output_file_path, "w") as f:
    json.dump(response_data, f, indent=4)

print(f"All responses saved to {output_file_path}")

"""# Create different versions of output file with different methods of handling questions with no valid sub-questions"""

# create another file where the questions that had no valid sub-questions are skipped

import json
import os


# Define the paths
input_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_simplebaselinedefault_predictions.json"
output_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_skipnovalidsubquestions_skipnovalidsubquestions_prediction.json"

if os.path.exists(input_path):
    print(f"File found at {input_path}, processing...")

    # Read the subquestion_predictionsXX.json
    with open(input_path, "r") as f:
        data = json.load(f)

    formatted_data = [{"questionId": p['questionId'], "prediction": p['prediction']} for p in data if 'subquestionResponses' in p]

    with open(output_path, "w") as f:
        json.dump(formatted_data, f, indent=4)

    print(f"File successfully created at {output_path}")
else:
    print(f"File not found at {input_path}. Please check the path and try again.")

"""## In our final output, we give questions with no valid subquestions an empty string prediction since they failed to perform reasoning"""

# create the official file, where questions with no valid subquestions are given a blank prediction

import json
import os


# Define the paths
input_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_simplebaselinedefault_predictions6.json"
output_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_officialprediction6.json"

if os.path.exists(input_path):
    print(f"File found at {input_path}, processing...")

    # Read the subquestion_predictionsXX.json
    with open(input_path, "r") as f:
        data = json.load(f)

    official_data = []
    for p in data:
      if 'subquestionResponses' in p:
        official_data.append({"questionId": p['questionId'], "prediction": p['prediction']})
      else:
        official_data.append({"questionId": p['questionId'], "prediction": ""})

    with open(output_path, "w") as f:
        json.dump(official_data, f, indent=4)

    print(f"File successfully created at {output_path}")
else:
    print(f"File not found at {input_path}. Please check the path and try again.")

# create the official file with subquestions/answers, where questions with no valid subquestions are given a blank prediction

import json
import os


# Define the paths
input_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_simplebaselinedefault_predictions6.json"
output_path = "/content/drive/MyDrive/CIS 5300/Final Project/NLP_Final_Project/Milestone_4/redone_strong_baseline_predictions/redone_strong_baseline_predictions6.json"

if os.path.exists(input_path):
    print(f"File found at {input_path}, processing...")

    # Read the subquestion_predictionsXX.json
    with open(input_path, "r") as f:
        data = json.load(f)

    official_data = []
    for p in data:
      if 'subquestionResponses' in p:
        sub_questions = ""
        for i, qa in zip(range(len(p['subquestionResponses'])), p['subquestionResponses']):
          if i != 0:
            sub_questions += " "
          sub_questions += qa['subquestion']
          sub_questions += " "
          sub_questions += qa['response']
        official_data.append({"questionId": p['questionId'], "sub_questions": sub_questions, "prediction": p['prediction']})
      else:
        official_data.append({"questionId": p['questionId'], "error": p['error'], "sub_questions": "", "prediction": ""})

    with open(output_path, "w") as f:
        json.dump(official_data, f, indent=4)

    print(f"File successfully created at {output_path}")
else:
    print(f"File not found at {input_path}. Please check the path and try again.")