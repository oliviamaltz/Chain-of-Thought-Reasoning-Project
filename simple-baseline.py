# -*- coding: utf-8 -*-
"""llava_simple_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12njD4JENhnNeuLleIlwdi0dx4MoIlk4G
"""

!pip install transformers
!pip install transformers torch torchvision accelerate sentencepiece

import torch
print(torch.__version__)

import torch
print(torch.cuda.is_available())

# Ensure necessary imports
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

# Specify the model identifier
model_id = "llava-hf/llava-1.5-7b-hf"

# Load the processor and model
processor = AutoProcessor.from_pretrained(model_id)
model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

print(model)

from PIL import Image
import requests
from io import BytesIO

from google.colab import drive
drive.mount('/content/drive')

test_image1 = Image.open("/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/test_images/90.jpg")
prompt = "USER: <image>\nWhat do you see in this image?\nASSISTANT:"


inputs = processor(images=test_image1, text=prompt, return_tensors="pt").to("cuda")
outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
)

response = processor.decode(outputs[0], skip_special_tokens=True)
print(response)

#Length penalty
import json
from PIL import Image

# Step 1: Load the question file (replace with the path to your questions JSON file)
question_file_path = "/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/selected_test_questions.json"

with open(question_file_path, "r") as f:
    questions = json.load(f)

# Step 2: Initialize a dictionary to store predictions
llava_predictions = {}

# Step 3: Process all questions
for i, question in enumerate(questions):  # Process all questions
    image_id = question["imageId"]
    question_text = question["question"]

    # Load the corresponding image
    image_path = f"/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/test_images/{image_id}.jpg"

    try:
        image = Image.open(image_path)
    except FileNotFoundError:
        print(f"Image for image ID {image_id} not found. Skipping.")
        continue

    # Create the prompt for LLaVA
    prompt = f"USER: <image>\n{question_text} Provide a short answer.\nASSISTANT:"
    inputs = processor(images=image, text=prompt, return_tensors="pt").to("cuda")

    # Generate the output
    outputs = model.generate(**inputs, max_new_tokens=50)

    # Decode and clean the predicted answer
    decoded_output = processor.decode(outputs[0], skip_special_tokens=True)
    predicted_answer = decoded_output.split("ASSISTANT:")[-1].strip()  # Extract only the assistant's response

    # Store the prediction in the dictionary
    llava_predictions[image_id] = predicted_answer
    print(f"Processed question {i + 1}/{len(questions)}: {image_id} -> {predicted_answer}")

# Step 4: Save all predictions to a JSON file
output_file_path = "/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/simple_baseline_predictions.json"
with open(output_file_path, "w") as f:
    json.dump(llava_predictions, f, indent=4)

print(f"Predictions for all questions saved to {output_file_path}")

#No length penalty
import json
from PIL import Image

# Step 1: Load the question file (replace with the path to your questions JSON file)
question_file_path = "/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/selected_test_questions.json"

with open(question_file_path, "r") as f:
    questions = json.load(f)

# Step 2: Initialize a list to store predictions
llava_predictions = []

# Step 3: Process all questions
for i, question in enumerate(questions):  # Process all questions
    image_id = question["imageId"]
    question_id = image_id  # Use image_id as the question_id
    question_text = question["question"]

    # Load the corresponding image
    image_path = f"/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/test_images/{image_id}.jpg"

    try:
        image = Image.open(image_path)
    except FileNotFoundError:
        print(f"Image for image ID {image_id} not found. Skipping.")
        continue

    # Create the prompt for LLaVA

    prompt = f"USER: <image>\n{question_text} Provide a short answer.\nASSISTANT:"
    inputs = processor(images=image, text=prompt, return_tensors="pt").to("cuda")

    # Generate the output
    outputs = model.generate(**inputs, max_new_tokens=50)

    # Decode and clean the predicted answer
    decoded_output = processor.decode(outputs[0], skip_special_tokens=True)
    predicted_answer = decoded_output.split("ASSISTANT:")[-1].strip()  # Extract only the assistant's response

    # Add the prediction to the list
    llava_predictions.append({
        "questionId": question_id,  # Use image_id as question_id
        "prediction": predicted_answer
    })
    print(f"Processed question {i + 1}/{len(questions)}: {question_id} -> {predicted_answer}")

# Step 4: Save all predictions to a JSON file
output_file_path = "/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/simple_baseline_predictions0.json"
with open(output_file_path, "w") as f:
    json.dump(llava_predictions, f, indent=4)

print(f"Predictions for all questions saved to {output_file_path}")

#length penalty
import json
from PIL import Image

# Step 1: Load the question file (replace with the path to your questions JSON file)
question_file_path = "/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/selected_test_questions.json"

with open(question_file_path, "r") as f:
    questions = json.load(f)

# Step 2: Initialize a list to store predictions
llava_predictions = []

# Step 3: Process all questions
for i, question in enumerate(questions):  # Process all questions
    image_id = question["imageId"]
    question_id = image_id  # Use image_id as the question_id
    question_text = question["question"]

    # Load the corresponding image
    image_path = f"/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/test_images/{image_id}.jpg"

    try:
        image = Image.open(image_path)
    except FileNotFoundError:
        print(f"Image for image ID {image_id} not found. Skipping.")
        continue

    # Create the prompt for LLaVA
    prompt = f"USER: <image>\n{question_text} Answer the question using a single word or phrase.\nASSISTANT:"
    inputs = processor(images=image, text=prompt, return_tensors="pt").to("cuda")

    # Generate the output with a length penalty of -1
    outputs = model.generate(**inputs, max_new_tokens=50, length_penalty=-1)

    # Decode and clean the predicted answer
    decoded_output = processor.decode(outputs[0], skip_special_tokens=True)
    predicted_answer = decoded_output.split("ASSISTANT:")[-1].strip()  # Extract only the assistant's response

    # Add the prediction to the list
    llava_predictions.append({
        "questionId": question_id,  # Use image_id as question_id
        "prediction": predicted_answer
    })
    print(f"Processed question {i + 1}/{len(questions)}: {question_id} -> {predicted_answer}")

# Step 4: Save all predictions to a JSON file
output_file_path = "/content/drive/My Drive/GQA_Project/NLP_Milestone1_Data_GoodOne/simple_baseline_predictions2.1.json"
with open(output_file_path, "w") as f:
    json.dump(llava_predictions, f, indent=4)

print(f"Predictions for all questions saved to {output_file_path}")

